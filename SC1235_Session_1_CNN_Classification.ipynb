{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter notebook.  The most important keyboard shortcuts (cf. the \"Help\" menu) are\n",
    "* **cursor keys** to select cells\n",
    "* **Enter** to go from command mode to edit mode (for changing cell contents)\n",
    "  * (**Esc** would go back to command mode.)\n",
    "* **Shift+Enter** to *execute and advance* a cell\n",
    "  * While experimenting with different values in the same cell, **Ctrl+Enter** is also handy, which executes but does not advance the cursor.\n",
    "* There is an edit mode with a green bar to the left, and a execution/command mode with a blue bar.\n",
    "* In command mode, some keys have a function:\n",
    "    * `l`: toggle line numbers\n",
    "    * `a`: new cell above \n",
    "    * `b`: new cell below\n",
    "    * `h`: help / see more keyboard shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC1235 Introduction to Medical Image Analysis Using Convolutional Neural Networks\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First Experiments with Random Data\n",
    "* Start by importing some required python modules that implement the layers we will use to build the network. \n",
    "* We also need a \"container\" to connect the layers: the \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random data\n",
    "\n",
    "In these examples, we'll use artificial data first, and then switch to real data.\n",
    "\n",
    "Run the code in the following box, which will create a pair of input data `x_train` and corresponding output data `y_train` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INST = 100\n",
    "x_train = np.random.random((NUM_INST, 1000)) # NUM_INST instances with 1000 random features\n",
    "y_train = np.zeros((NUM_INST,)) # Label vector (initialized with 0s)\n",
    "y_train[:int(NUM_INST/2)] = 1 # set first half of vector to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape = (1000,)))\n",
    "model.add(Dense(units=256, name=\"Hidden\")) # Play with the number of units (==neurons)\n",
    "# Optionally increase the number of layers.\n",
    "#model.add(Dense(units=128))\n",
    "#model.add(Dense(units=64))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "model.summary()\n",
    "# If only interested in the number of parameters, use this:\n",
    "# print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=10, epochs=100)\n",
    "# Clicking left to the output once will change the display mode from a scrollable field to a full display and back. Double-clicking it collapses it, so it is not so dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A remark on optimization\n",
    "* Optimizers like SGD, ADAM, ADAGrad ADADelta etc. are variants of Stochastic Gradient Descent (SGD).\n",
    "* SGD estimates the gradient for parameters based on a batch of examples.\n",
    "    * The larger the batch, the better the estimated gradiend approximates the gradient for the whole dataset.\n",
    "* It takes about 300 epochs to converge when creating 1000 instances.\n",
    "\n",
    "### Quiz: Interpreting the result\n",
    "* What can you observe regarding the loss?\n",
    "* Why is that possible?\n",
    "* Change the number of training instances to 1000. Assure that the classes are equally frequent again. What can you observe?\n",
    "* Be reminded that you have to re-create the model to reset the weights. To do this, execute the cell with the model definition (important is the `model.compile()` call)\n",
    "\n",
    "### Investigate the \"history\" object you created\n",
    "* Try out the following commands and inspect the variables.\n",
    "* Make use of tab completion, e.g. by typing `hidden_layer.<tab>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = history.history['loss']\n",
    "weights = history.model.get_weights()\n",
    "hidden_layer = history.model.get_layer(\"Hidden\")\n",
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Classification: _MNIST handwritten digits_\n",
    "\n",
    "### Read the data\n",
    "\n",
    "* We want to work on images: MNIST, which we import and load next.\n",
    "* You can import them from Keras with one line, because it is one of the standard datasets used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you execute this cell, you will overwrite the data above.\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reduce data by factor 10 / 20 for fast execution during course\n",
    "x_train = x_train[::10]\n",
    "y_train = y_train[::10]\n",
    "x_test = x_test[::20]\n",
    "y_test = y_test[::20]\n",
    "\n",
    "# verify resulting array shapes\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data\n",
    "\n",
    "Look at the shape of the `x_train` variable to understand how the data is organised.\n",
    "* You can see that the data has 60.000 examples, each of shape 28x28.\n",
    "* These are images... of size 28x28 pixels.\n",
    "* The corresponding output is just a long vector of corresponding labels in the range [0...9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the shape of x_train\n",
    "print(x_train.shape, y_train.shape, y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with *images* now, we want to display them.\n",
    "* `matplotlib` is a python package well suited plotting data and displaying images. Let's import it.\n",
    "* Then, load and display one of the images \"inline\" in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an image\n",
    "plt.imshow(x_train[600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from displaying images, `matplotlib` also helps you visualise logs. Below, we display the learning success as measured by the loss. Remember that the loss is in the `history` object created while fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we could be interested in the distribution of labels in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the labels for a classification network\n",
    "We want to convert the numeric labels to so-called *\"one-hot vectors\"*.\n",
    "* One-hot means that the network does not directly output a number between 0 and 9 representing the digit.\n",
    "* Rather, we want a vector with 10 dimensions, in which only one entry is 1, all others 0, e.g. `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]` to label a \"2\".\n",
    "* *Rationale:* The digits represent different categorical classes, and we want to penalize all confused digits the same; it is not \"better\" or \"closer\" if the network outputs 4.2 given an image depicting a \"6\" than if the output is 1.\n",
    "* In general, the one-hot encoding helps with classification problems and allows to let the neuron with maximal activation \"win\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "# Code to convert labels\n",
    "y_train_one_hot = (np.arange(num_labels) == y_train[:,np.newaxis]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras offers a convenience function to achieve the same:\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_labels)\n",
    "# Same for the testing data\n",
    "y_test_one_hot  = to_categorical(y_test, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification with a simple neural network\n",
    "We now want to train the above network on this data. We have to adapt it to use inputs of a different shape, and to produce vector outputs. We have prepared this below:\n",
    "* Modified the parameter `input_shape=(...)` to adapt to the new data\n",
    "* Modified the number of dense units in the output layer to reflect the number of classes; 10 in the digits example\n",
    "* Modified the loss function to deal with multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(28,28)))\n",
    "model.add(Flatten()) # Layer reshaping the 28x28 arrays into vectors of length 28*28=784\n",
    "model.add(Dense(units=256)) # Try higher or lower numbers of hidden units!\n",
    "# Try adding more layers!\n",
    "model.add(Dense(units=128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This experiments takes about 1 sec per epoch on an older MacBook Pro.\n",
    "history = model.fit(x_train, y_train_one_hot, batch_size=500, epochs=100) # In this example, you'll no longer want batches of size 10..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on independent test data\n",
    "* The following cell executes the model on the test data\n",
    "* The result is a list of 10-vectors (recall the on-hot encoding), only this time there are also values between 0 and 1.\n",
    "* How can we compare these with the true labels in `y_test_one_hot`? There are many possible ways to evaluate classifiers; in general, you want to define some kind of error, usually based on differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "print(x_test.shape, pred.shape)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `argmax()` function may come in handy, which converts from the one-hot representation back to integer indices of the maximally activated classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image classification with a simple convolutional neural network (CNN)\n",
    "For an introduction into convolutional layers, see course notes.\n",
    "\n",
    "### Weight sharing\n",
    "Exploring convolutions with and without weight sharing.\n",
    "* First, your input data now needs to have a \"channel\" dimension, as the convolutional filter result will be a multi-channel image.\n",
    "* Next, you will need to remove the 2D nature again to feed into dense layers. \n",
    "  * `Flatten()` does this for you.\n",
    "  * Train the network as before.\n",
    "  * What do you observe?\n",
    "\n",
    "Later, we'll explore how a convolutional layer without weight sharing affects the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = Sequential()\n",
    "convnet.add(InputLayer(input_shape=(28,28,1)))\n",
    "convnet.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(MaxPool2D())\n",
    "convnet.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "convnet.add(MaxPool2D())\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(units=128))\n",
    "convnet.add(Dropout(0.5))\n",
    "convnet.add(Dense(units=10, activation='softmax'))\n",
    "convnet.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "print(\"convnet parameters: {0:,}\".format(convnet.count_params()))\n",
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convnet_history = convnet.fit(x_train[...,np.newaxis], y_train_one_hot, batch_size=500, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Plot the loss (history object, see above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have scikit-learn installed (try `conda install scikit-learn`), that offers utility methods for computing evaluation metrics such as a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "pred = convnet.predict(x_test[...,np.newaxis])\n",
    "cm = sklearn.metrics.confusion_matrix(pred.argmax(axis = -1), y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more intuitive to look at it as a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: Numpy and Matplotlib are two important, central libraries for numeric computing with Python. In addition, there are also more advanced libraries such as Seaborn, which build upon the things introduced above and offer dedicated functions for complex graphics, such as a combined version of the above matrix + heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without weight sharing\n",
    "\n",
    "Now, let's try convolution without weight sharing.\n",
    "* Use `LocallyConnected2D` for this. \n",
    "* What do you observe? Try training the network.\n",
    "* Regard the number of parameters. Change the network, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_ws = Sequential()\n",
    "without_ws.add(InputLayer(input_shape=(28,28,1)))\n",
    "without_ws.add(LocallyConnected2D(32, kernel_size=(3,3)))\n",
    "#without_ws.add(LocallyConnected2D(32, kernel_size=(3,3)))\n",
    "without_ws.add(MaxPool2D())\n",
    "without_ws.add(LocallyConnected2D(32, kernel_size=(3,3)))\n",
    "#without_ws.add(LocallyConnected2D(32, kernel_size=(3,3)))\n",
    "without_ws.add(MaxPool2D())\n",
    "without_ws.add(Flatten())\n",
    "without_ws.add(Dense(units=128))\n",
    "without_ws.add(Dropout(0.5))\n",
    "without_ws.add(Dense(units=10, activation='softmax'))\n",
    "without_ws.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "print(\"without_ws parameters: {0:,}\".format(without_ws.count_params()))\n",
    "without_ws.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wws_history = without_ws.fit(np.reshape(x_train, x_train.shape+(1,)), y_train_one_hot, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wws_history.history['loss'])\n",
    "plt.show()\n",
    "import sklearn.metrics\n",
    "pred = without_ws.predict(x_test[...,np.newaxis])\n",
    "cm = sklearn.metrics.confusion_matrix(pred.argmax(axis = -1), y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From CNN to FCN\n",
    "* In the following, explore how a network with dense layers is equivalent to a properly configured fully-convolutional network\n",
    "* Flattening is replaced by a convolutional layer whose kernel spans the full size of the previous output.\n",
    "    * Replace the `Flatten` layer and subsequent `Dense` layers by `Conv2D` layers.\n",
    "    * Note, that a `Flatten` layer is still required to convert into the output vector representation.\n",
    "* Convince yourself that the number of trainable parameters is indeed unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcn = Sequential()\n",
    "fcn.add(InputLayer(input_shape=(None,None,1)))\n",
    "fcn.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "fcn.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "fcn.add(MaxPool2D())\n",
    "fcn.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "fcn.add(Conv2D(32, kernel_size=(3,3), padding='same'))\n",
    "fcn.add(MaxPool2D())\n",
    "fcn.add(Conv2D(128, kernel_size=(7,7), padding='valid'))\n",
    "fcn.add(Dropout(0.5))\n",
    "fcn.add(Conv2D(10, kernel_size=(1,1), activation='softmax'))\n",
    "fcn.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "print(\"fcn parameters: {0:,}\".format(fcn.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_history = fcn.fit(x_train[...,np.newaxis],\n",
    "                      y_train_one_hot[:,np.newaxis,np.newaxis,:],\n",
    "                      batch_size=500, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fcn_history.history['loss'])\n",
    "plt.show()\n",
    "pred = fcn.predict(x_test[...,np.newaxis])[:,0,0]\n",
    "cm = sklearn.metrics.confusion_matrix(pred.argmax(axis = -1), y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Visualize this confusion matrix, compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This code generates the table of contents at the top of the notebook\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
