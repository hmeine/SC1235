{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SC1235 Introduction to Medical Image Analysis Using Convolutional Neural Networks\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Preparing for Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, LocallyConnected2D\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a GPU is indeed available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it is useful to be able to check that tensorflow (which is used internally by keras here) is able to see a real GPU. If tensorflow falls back to the CPU, your code will still work, but be *much* slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "# print all devices visible to tensorflow \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first thing one needs to prepare is how to access the data:\n",
    "* separation of training & test data\n",
    "* random access to parts of the data (e.g. single images out of a large database)\n",
    "* random permutations (shuffling)\n",
    "\n",
    "In this notebook, we will just load data that we have prepared.  In practice, one might be interested in\n",
    "* [pydicom](https://pydicom.github.io) for loading DICOM image data\n",
    "* [HDF5 and h5py](https://www.h5py.org) for efficient storage of large binary data\n",
    "* [numpy.random.permutation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html) comes in very handy for shuffling large arrays\n",
    "\n",
    "> If you are already capable of python programming, here are some more hints how you could improve on the approach of this notebook:\n",
    "> * Create a generator function that loads one batch of size `batch_size` and returns it.\n",
    "> * Track which images you have already drawn, so that you can start over after one epoch (one epoch is one run through all images) with a fresh permutation.\n",
    "> * Call the `fit_generator()` function instead of `fit()`.  Read the Keras documentation for this at [keras.io](http://keras.io).\n",
    ">\n",
    "> If you still have time and motivation, implement the same using a HDF5 file storage rather than plain disk storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "We have prepared a set of abdominal CT slices, strongly downsampled, completely anonymized, together with segmentation labels which we will inspect shortly.  The following cell just downloads the data to the machine this notebook runs on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!test -e tmp_slices.npz || curl -L \"https://drive.google.com/uc?export=download&id=1R2-H0dhhrj6XNK7Q-MazIWGeFDOf6Zya\" --output tmp_slices.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into separate training and test sets.\n",
    "\n",
    "* Training converges with about 200 slices.\n",
    "* The initial results when training with 700 slices are terrible. (Exercise: Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SLICE_COUNT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('tmp_slices.npz')\n",
    "\n",
    "x_train = loaded['x_train'][:TRAINING_SLICE_COUNT]\n",
    "y_train = loaded['y_train'][:TRAINING_SLICE_COUNT]\n",
    "\n",
    "x_test = loaded['x_train'][TRAINING_SLICE_COUNT:]\n",
    "y_test = loaded['y_train'][TRAINING_SLICE_COUNT:]\n",
    "\n",
    "assert len(x_train) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Demo Data\n",
    "The PNGs used for this hands-on contain abdominal CT scans showing the liver, resampled to 4mm voxel size and after applying a liver HU window (centered at 20 HU, width 450 HU).  The corresponding masks contain values 0 (background), 1 (liver), 2..3 (different classes of lesions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_test_slice = 1800\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (9, 5))\n",
    "imgplot = ax[0].imshow(x_test[example_test_slice])\n",
    "ax[0].set_title('orig')\n",
    "imgplot = ax[1].imshow(y_test[example_test_slice])\n",
    "ax[1].set_title('mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will start with a binary segmentation problem (0: background, 1: liver), so we will remove the lesion labels / turn them into liver (value 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the lesion labels (values 2..3)\n",
    "y_train_binary = y_train.clip(0, 1)\n",
    "y_test_binary = y_test.clip(0, 1)\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (9, 5))\n",
    "imgplot = ax[0].imshow(x_test[example_test_slice])\n",
    "ax[0].set_title('orig')\n",
    "imgplot = ax[1].imshow(y_test_binary[example_test_slice])\n",
    "ax[1].set_title('mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Segmentation: Auto Encoder (AE)-Style\n",
    "* We define short functions that return a model.\n",
    "* The first is a simple architecture that collapses and expands an image into the desired mask, similar to an Auto Encoder (AE).\n",
    "* The second is the famous U-Net.\n",
    "\n",
    "Further reading: Variational AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getModel(_filters=32, filters_add=0, _kernel_size=(3,3), _padding='same', _activation='relu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid'):\n",
    "    model = Sequential()\n",
    "    # We are indifferent about the xy size, but accept only one channel (gray value images). This has the consequence that debugging sizes gets harder.\n",
    "    model.add(InputLayer(input_shape=(None,None,1))) \n",
    "    \n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer'))\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+2*filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters+filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "    model.add(Conv2D(filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer))\n",
    "\n",
    "    model.add(Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution Mode 'valid' vs. 'same' / Automatic Padding\n",
    "\n",
    "In case you use convolutions with `'valid'` padding, the input size needs to be padded before submitting so that still an output corresponding to the full slice (or the patch size, respectively) is generated. The number of pixels to pad depends on the network architecture. There is an excellent technical report detailing the arithmetics to calculate the receptive field and required padding from a network definition at https://arxiv.org/pdf/1603.07285.pdf. Another description is at https://medium.com/@Synced/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-42f33d4378e0.\n",
    "\n",
    "We will explore the difference later in the context of _overlapping tiles segmentation_.\n",
    "The function is somewhat hard-coded for the network architecture above: it pads the input with a fixed amount of voxels in case the model gives an output that is differently sized from the inmput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pad_image_for_model(model, input_image):\n",
    "    '''Determine the necessary amount of padding\n",
    "    (difference between input and output size of the model)\n",
    "    and apply it to an ndarry with one or more images.'''\n",
    "    \n",
    "    padding = 0\n",
    "    if model.get_layer('firstConvolutionalLayer').padding == 'valid':\n",
    "        padding = 20 # WARNING: Hard-coded for above architecture!\n",
    "\n",
    "        # determine in which dimension to apply this padding\n",
    "        ndim_padding = []\n",
    "        if np.ndim(input_image) > 2:\n",
    "            # do not pad along batch dimension (if present)\n",
    "            ndim_padding.append((0, 0))\n",
    "        ndim_padding.append((padding, padding)) # pad above/below image (y dimension)\n",
    "        ndim_padding.append((padding, padding)) # pad left/right of image (x dimension)\n",
    "        if np.ndim(input_image) > 3:\n",
    "            # do not pad along channel dimension (if present)\n",
    "            ndim_padding.append((0, 0))\n",
    "        \n",
    "        input_image = np.lib.pad(input_image, ndim_padding,\n",
    "                                 #'constant', constant_values = 0)\n",
    "                                 'reflect')\n",
    "\n",
    "    return input_image, padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our small data, it is possible to train a classifier that takes full slices. However, when dealing with input of arbitrary size, this will no longer work. Images have to be partitioned, and the individual results need to be stitched into the final output. This only works when padding the input with blank or other voxels to achieve the desired output size.\n",
    "\n",
    "### Experiment 1: 'valid' Convolutions\n",
    "\n",
    "Get a model with padding of 'valid', i.e. the size will shrink and a proper amount of voxels need to be added prior to processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelValid = getModel(_padding = 'valid')\n",
    "\n",
    "modelValid.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(modelValid.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: 'same' Convolutions\n",
    "\n",
    "Get a model with `same` padding, i.e. the input is padded with zeros before each convolution such that an output size equal to the input is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelSame = getModel()\n",
    "\n",
    "modelSame.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(modelSame.count_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing code during training\n",
    "\n",
    "This is a small interludium section that isn't directly related to the experiments around the overlapping tile concept.\n",
    "\n",
    "In Keras (and also other frameworks), code can be executed with every iteration/batch/... Keras makes this particularly easy by offering \"callbacks\" where you can override one or more functions to execute your code. We will use this to generate images of the learning success after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class VisualHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # also show initial prediction\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # show prediction after every training epoch\n",
    "        plot_prediction(self.model, example_test_slice)\n",
    "        \n",
    "vh_callback = VisualHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once a model is trained, you'll want to evaluate it. Predicting using a given model and then plotting one slice is implemented in this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def do_prediction(model, input_image, verbose = False):\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    if verbose:\n",
    "        print(\"input shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    if verbose:\n",
    "        print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    return input_image, input_array, y_predicted, padding\n",
    "\n",
    "def plot_prediction(model, pred_slice_index):\n",
    "    # get single slice\n",
    "    input_image    = x_test[pred_slice_index]\n",
    "    # could use y_train_binary here for the first half of the notebook, but in the end we want to see the lesion\n",
    "    reference_mask = y_test[pred_slice_index]\n",
    "\n",
    "    input_image, input_array, y_predicted, padding = do_prediction(model, input_image)\n",
    "    \n",
    "    padded_extent = np.array([0, input_array.shape[2], input_array.shape[1], 0]) - 0.5 - padding\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 5 if padding else 4, figsize = (11 if padding else 8, 3), sharey = True)\n",
    "    ax[0].imshow(x_test[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0])\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-3].imshow(reference_mask.clip(0,1))\n",
    "    ax[-3].set_title('reference mask')\n",
    "    ax[-1].imshow(reference_mask.clip(0,1) - y_predicted[0,:,:,0])\n",
    "    ax[-1].set_title('(ref - predicted)')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training\n",
    "* As the arrays we created before are 3-dimensional (no channel for grey images), we have to add one dimension to make it compatible with the ConvNet.\n",
    "* About 100 epochs lead to a pretty well-performing net. On an average CPU, one iteration takes about 10-15 sec. On GPU, this is much faster (increase the batch size also, to avoid unneccessary GPU memory transfers)\n",
    "* With Batch Normalisation and PReLU, the number of parameters gets much larger, and training takes much longer. \n",
    "    * Does the result warrant the wait?\n",
    "    * Explain!\n",
    "* Callbacks enable better logging. \n",
    "    * We can add the TensorBoard logging mechanism. \n",
    "    * TensorBoard needs to be started externally, pointing to the log directory, which defaults to `./logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'same'\n",
    "historySame = modelSame.fit(x_train[...,np.newaxis],\n",
    "                            y_train_binary[...,np.newaxis],\n",
    "                            batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for _padding = 'valid' and 'reflect' padding\n",
    "historyValid = modelValid.fit(np.lib.pad(x_train[...,np.newaxis],\n",
    "                                         [(0,0), (20,20), (20,20), (0,0)], 'reflect'),\n",
    "                              y_train_binary[...,np.newaxis],\n",
    "                              batch_size=20, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction\n",
    "Let's look at the prediction from some more example slices, but let's only use the `x_test` slices that we did not use for training. (In a real scenario, we would to the separation of training & test data on the level of patients, *before* extracting slices, and we'd also have a validation set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_indices = np.random.choice(x_test.shape[0], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with 'valid' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelValid, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with 'same' convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for a in slice_indices:\n",
    "    plot_prediction(modelSame, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile-based Prediction\n",
    "\n",
    "Let's predict the image divided into upper and lower half, and then in full. Note that the network could also predict smaller or larger tile sizes.  Such an approach is necessary for large images, or with volumetric data and 3D convolutions, when it is not possible to have the whole image in (GPU) memory at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-picked slice useful to visualize benefits of tiled approach\n",
    "example_test_slice = 505 - TRAINING_SLICE_COUNT\n",
    "assert example_test_slice > 0, 'when increasing TRAINING_SLICE_COUNT, please pick a new example slice as well'\n",
    "\n",
    "\n",
    "def plot_prediction_tiled(model, pred_slice_index, tile = None):\n",
    "    # get single slice\n",
    "    input_image    = x_train[pred_slice_index]\n",
    "    reference_mask = y_train_binary[pred_slice_index]\n",
    "\n",
    "    # first do padding of full slice\n",
    "    input_image, padding = pad_image_for_model(model, input_image)\n",
    "    \n",
    "    # add batch and channel dimensions (network expects 4D arrays)\n",
    "    input_array = input_image[np.newaxis,:,:,np.newaxis]\n",
    "    print(\"padded input shape:\", input_array.shape)\n",
    "\n",
    "    # then cut the specified tile box (plus padding)\n",
    "    if tile is not None: # (default is full slice)\n",
    "        tile = np.asarray(tile)\n",
    "        assert tile.shape == (2, 2)\n",
    "        padded_tile = tile.copy()\n",
    "        padded_tile[:,1] += 2*padding\n",
    "        input_array = input_array[:,\n",
    "                                  padded_tile[0][0]:padded_tile[0][1],\n",
    "                                  padded_tile[1][0]:padded_tile[1][1],\n",
    "                                  :]\n",
    "        reference_mask = reference_mask[tile[0][0]:tile[0][1],\n",
    "                                        tile[1][0]:tile[1][1]]\n",
    "        print(\"tiled padded shape:\", input_array.shape)\n",
    "\n",
    "    y_predicted = model.predict(input_array)\n",
    "    print(\"output shape:\", y_predicted.shape)\n",
    "\n",
    "    padded_extent = np.array([0,input_array.shape[2],input_array.shape[1],0]) - 0.5 - padding\n",
    "    orig_extent = np.array([0,reference_mask.shape[1],reference_mask.shape[0],0]) - 0.5\n",
    "    if tile is not None:\n",
    "        padded_extent[:2] += tile[1,0]\n",
    "        padded_extent[2:] += tile[0,0]\n",
    "        orig_extent[:2] += tile[1,0]\n",
    "        orig_extent[2:] += tile[0,0]\n",
    "\n",
    "    # display prediction for inspection\n",
    "    f, ax = plt.subplots(1, 4 if padding else 3, figsize = (14 if padding else 12, 3), sharey = True)\n",
    "    ax[0].imshow(x_train[pred_slice_index])\n",
    "    ax[0].set_title('orig')\n",
    "    if padding:\n",
    "        ax[1].imshow(input_array[0,:,:,0], extent = padded_extent)\n",
    "        ax[1].set_title('padded input')\n",
    "    ax[-2].imshow(y_predicted[0,:,:,0], extent = orig_extent)\n",
    "    ax[-2].set_title('predicted mask')\n",
    "    ax[-1].imshow(reference_mask, extent = orig_extent)\n",
    "    ax[-1].set_title('reference mask')\n",
    "    ax[0].set_ylim(*padded_extent[2:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tile = [[0,42],[0,76]]\n",
    "plot_prediction_tiled(modelSame, example_test_slice, tile)\n",
    "tile = [[42,76],[0,76]]\n",
    "plot_prediction_tiled(modelSame, example_test_slice, tile)\n",
    "plot_prediction(modelSame, example_test_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = [[0,42],[0,76]]\n",
    "plot_prediction_tiled(modelValid, example_test_slice, tile)\n",
    "tile = [[42,76],[0,76]]\n",
    "plot_prediction_tiled(modelValid, example_test_slice, tile)\n",
    "plot_prediction(modelValid, example_test_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularisation\n",
    "* Regularisation should improve convergence. Let's try and add some Batch Normalisation first. Batch Normalisation intends to normalise the input to a (convolutional) layer, so that the values in the resulting feature maps don't get exessively large. \n",
    "    * Keras offers BatchNorm layers.\n",
    "    * Include one before each convolutional layer.\n",
    "* Another regularisation measure is to choose better activation functions. Probabilistic Rectified Linear Units (PReLU) crop negative values to a small epsilon, but route through any value greater than zero.\n",
    "    * In Keras, activation functions are selected through the `activation=[softmax|elu|selu|relu|tanh|sigmoid|hard_sigmoid|linear]` parameter to a layer, each in quotes.\n",
    "    * PReLU is one of the advanced activation functions that need to be added as a layer. It has many \"trainable\" parameters. How many? Why?\n",
    "    * How many parameters does ReLU have?\n",
    "* Lastly, L1 and L2 norm can be used as additional constraints on weights, biases, and activations.\n",
    "    * In Keras, this normalisation is again a parameter to the layer initialisation, using `kernel_regularizer=[l1(0.01)|l2(0.01)|l1_l2(0.01)]`.\n",
    "    * You have to `from keras.regularizers import l1, l2, l1_l2` to enable this functionality.\n",
    "    * You can also explore `bias_regularizer` and `activity_regularizer`.\n",
    "* Make your life easier by extracting a block: Conv -- Relu -- Batch Normalization. Then play with the options -- but carefully: When you just switch everything \"on\", chances are you will get bad results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use the following block to generate the regularisation block\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "def addConvBN(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name = None):\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    if activation == 'prelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(PReLU())\n",
    "    elif activation == 'lrelu':\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name = name))\n",
    "        model.add(LeakyReLU())\n",
    "    else:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name = name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch norm model\n",
    "def getBNModel(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _num_classes=1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # this is really ugly, but TensorFlow's batch normalization\n",
    "    # currently has a limitation that it cannot work on unknown input sizes,\n",
    "    # so we need to get the height & width of our training data:\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        model.add(InputLayer(input_shape = (h+40, w+40, 1)))\n",
    "    elif _padding == 'same':\n",
    "        model.add(InputLayer(input_shape = (h, w, 1)))\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, name='firstConvolutionalLayer')\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    model.add(UpSampling2D())\n",
    "\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "    addConvBN(model, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer)\n",
    "\n",
    "    model.add(Conv2D(_num_classes, kernel_size=(1,1), activation=_final_layer_nonlinearity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Norm with PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This network uses the PReLU layer.\n",
    "# Note the number of parameters when executing model.summary().\n",
    "modelValidBN = getBNModel(_padding='valid')\n",
    "modelValidBN.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "modelValidBN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(modelValidBN, x_train[...,np.newaxis])\n",
    "historyValidBN = modelValidBN.fit(padded_x_train,\n",
    "                                  y_train_binary[...,np.newaxis],\n",
    "                                  batch_size=10, epochs=5, callbacks=[vh_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.plot(historyValid.history['loss'], label = 'without BN')\n",
    "ax.plot(historyValidBN.history['loss'], label = 'with batch normalization')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid()\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use padded convolutions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSameBN = getBNModel(_padding='same')\n",
    "modelSameBN.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "history = modelSameBN.fit(x_train[...,np.newaxis], y_train_binary[...,np.newaxis],\n",
    "                          batch_size=10, epochs=5, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Norm with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Now compare with a model with ReLU (instead of PReLu)\n",
    "model = getBNModel(_padding='valid', _activation='relu')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train, y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularisation using L1/L2 norm on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "model = getModel(_padding='valid', _kernel_regularizer = l2(0.001))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Loss functions\n",
    "* Loss functions take the predicted output, `y_pred`, and the expected output, `y_train`, and calculate their distance. The result is the minimization target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss functions: Final layer non-linearity dependency\n",
    "* We have worked with binary crossentropy. See [this blog series](http://neuralnetworksanddeeplearning.com/chap3.html) for a comment:\n",
    "    > When should we use the cross-entropy instead of the quadratic cost?<p>In fact, the cross-entropy is nearly always the better choice, provided the output neurons are sigmoid neurons.\n",
    "* Experiment with different loss functions: `mean_squared_error | logcosh | binary_crossentropy | cosine_proximity`\n",
    "* Experiment with different final layer nonlinearities: `softmax | elu | selu | relu | tanh | sigmoid | hard_sigmoid | linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Functions: Jaccard\n",
    "Generally considered a powerful loss is also Jaccard loss; it provides larger errors and therefore more stable gradients close to the solution. $l_j = \\frac{\\sum |A*B|}{\\sum |A| +\\sum |B| -\\sum |A*B|}$\n",
    "\n",
    "In practice, we also have to prevent division by zero. The following code uses a smoothing term to avoid exploding or disapearing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    The jaccard distance loss is useful for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train model with Jaccard loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = getModel()\n",
    "model.compile(loss=jaccard_distance_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train, \n",
    "                    y_train_binary[...,np.newaxis], \n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Functions: Dice\n",
    "* For segmentation, the Dice loss is also very common. $l_d = 2*\\sum \\frac{|A*B|} {\\sum A^2 + \\sum B^2}$\n",
    "\n",
    "NB: Jaccard and Dice are very similar overlap measures and can easily be computed from each other (bijection):\n",
    "<img src=\"images/jaccard_vs_dice.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    @url: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train model with Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = getBNModel()\n",
    "model.compile(loss=dice_coef_loss, optimizer='adadelta')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segmentation using a U-Net\n",
    "* U-Nets are characterized by a downsampling path and an upsamling path, which allow for a pixel-wise output.\n",
    "* Skip connections are used between them in order to make it easier for the network to retain fine details.\n",
    "* Below is a diagram of the U-Net we will create now. You'll learn how to create it, too.\n",
    "<img src=\"images/U-net_4_levels.png\" alt=\"U-Net diagram\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will use this to generate the regularisation block for the sequential model.\n",
    "def addConvBNSequential(model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None, name=None):\n",
    "    if batch_norm:\n",
    "        model = BatchNormalization()(model)\n",
    "    if activation == 'prelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = PReLU()(model)\n",
    "    elif activation == 'lrelu':\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "        model = LeakyReLU()(model)\n",
    "    else:\n",
    "        model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer, name=name)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Creates a small U-Net.\n",
    "from keras.layers import Input, concatenate\n",
    "def get_batchnorm_unet(_filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n",
    "\n",
    "    h, w = x_train.shape[1:]\n",
    "    if _padding == 'valid':\n",
    "        input_layer = Input(shape = (h+40, w+40, 1))\n",
    "    elif _padding == 'same':\n",
    "        input_layer = Input(shape = (h, w, 1))\n",
    "\n",
    "    x0 = addConvBNSequential(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm, name = 'firstConvolutionalLayer')\n",
    "    x0 = addConvBNSequential(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = MaxPool2D()(x0)\n",
    "    \n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x1 = addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = MaxPool2D()(x1)\n",
    "    \n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x2 = addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = UpSampling2D()(x2)\n",
    "    \n",
    "    x3 = concatenate([x1,x3])\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x3 = addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = UpSampling2D()(x3)\n",
    "    \n",
    "    x4 = concatenate([x0,x4])\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "    x4 = addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n",
    "\n",
    "    output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets increasingly interesting to plot the architecture.\n",
    "# (1) plotting to PNG image file\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='U-Net.png', show_shapes=False, show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename = 'U-Net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (2) plotting to SVG vector graphics format\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                    y_train_binary[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_batchnorm_unet(_activation='relu', _batch_norm=True)\n",
    "model.compile(loss=dice_coef_loss, optimizer='adam')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "history = model.fit(x_train[...,np.newaxis],\n",
    "                    y_train[...,np.newaxis],\n",
    "                    batch_size=10, epochs=75, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Label Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels into a one-hot representation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Convert to uint8 data and find out how many labels there are\n",
    "t = y_train.astype(np.uint8)\n",
    "t_max = int(np.max(y_test))\n",
    "print(\"Range of values: [0, {}]\".format(t_max))\n",
    "y_train_one_hot = to_categorical(t, num_classes=t_max+1).reshape((y_train.shape)+(t_max+1,))\n",
    "print(\"Shape before: {}; Shape after: {}\".format(y_train.shape, y_train_one_hot.shape))\n",
    "\n",
    "# The liver neuron should also be active for lesions within the liver\n",
    "liver = np.max(y_train_one_hot[:,:,:,1:], axis=3)\n",
    "y_train_one_hot[:,:,:,1] = liver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = getBNModel(_num_classes=4)\n",
    "model.compile(loss=dice_coef_loss, optimizer='adadelta')\n",
    "print(\"Model parameters: {0:,}\".format(model.count_params()))\n",
    "padded_x_train, padding = pad_image_for_model(model, x_train[...,np.newaxis])\n",
    "history = model.fit(padded_x_train,\n",
    "                    y_train_one_hot,\n",
    "                    batch_size=10, epochs=5, callbacks=[vh_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Assignment: Extend the plot function to handle multiple classes.\n",
    "Then, activate the visualization callback in the training again. Try to find a slice with more than one output class to see the success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This code generates the table of contents at the top of the notebook\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "livereveal": {
   "auto_select": "code",
   "auto_select_fragment": "code",
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "default",
   "transition": "linear"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
